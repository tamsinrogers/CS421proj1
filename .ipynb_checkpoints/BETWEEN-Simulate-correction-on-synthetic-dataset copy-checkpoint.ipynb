{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the generated synthetic data file and read it as a python dictionary called data\n",
    "\n",
    "import json\n",
    "\n",
    "file = open('synthetic_dataset_with_time.txt', 'r')\n",
    "text = file.read()\n",
    "data = json.loads(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emip_toolkit as EMTK\n",
    "\n",
    "# find areas of interest (AOIs) once again\n",
    "image_path = \"./\"\n",
    "image = \"synthetic_text.png\"\n",
    "aoi = EMTK.find_aoi(image, image_path, level=\"sub-line\")\n",
    "\n",
    "\n",
    "# add tokens to AOIs \n",
    "file_path = \"./\"\n",
    "aois_with_tokens = EMTK.add_tokens_to_AOIs(file_path, image.split(\".\")[0]+\".txt\", aoi)\n",
    "aois_with_tokens.head()\n",
    "\n",
    "\n",
    "import correction\n",
    "\n",
    "# find the y coordinate of each line in the text\n",
    "line_ys = correction.find_lines_Y(aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import drift_algorithms as algo\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw original correct fixations\n",
    "\n",
    "correction.draw_fixation('synthetic_text.png', data['robot1'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add regressions\n",
    "reg = correction.between(0.2, data['robot1'].copy())\n",
    "error_test = correction.error_noise(0.5, 30, 0.5, reg)\n",
    "\n",
    "# draw the trial with added error\n",
    "correction.draw_fixation('synthetic_text.png', error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the center of each word, we need this for warp algorithm\n",
    "word_centers = correction.find_word_centers(aoi)\n",
    "word_centers = np.array(word_centers.copy(), dtype=int)\n",
    "\n",
    "# warp correction\n",
    "np_array = np.array(error_test.copy(), dtype=int)\n",
    "durations = np.delete(np_array, 0, 1)\n",
    "durations = np.delete(durations, 0, 1)\n",
    "np_array = np.delete(np_array, 2, 1)\n",
    "\n",
    "# run warp\n",
    "warp_correction = algo.warp(np_array, word_centers)\n",
    "\n",
    "# this is a cool function that checks the quality of the correction\n",
    "percentage, match_list = correction.correction_quality(aoi, data['robot1'].copy(), warp_correction)\n",
    "print(percentage)\n",
    "\n",
    "# this is a cool function that draws the correction in red if the algorithm made a mistake\n",
    "correction.draw_correction('synthetic_text.png', warp_correction, match_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the example I wanted to show you of running an error generator, then correcting the data, then seeing how well the correction algorithm did.  You will need to repeat the same concept for a bunch of algorithms in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing algorithms with BETWEEN-LINE-REGRESSION error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "attach_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "chain_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "regress_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "warp_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "cluster_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "compare_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "merge_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "segment_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "split_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "stretch_results = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "word_centers = correction.find_word_centers(aoi)\n",
    "word_centers = np.array(word_centers.copy(), dtype=int)\n",
    "\n",
    "duration_word_centers = correction.find_word_centers_and_duration(aois_with_tokens)\n",
    "duration_word_centers = np.array(duration_word_centers.copy(), dtype=int)\n",
    "\n",
    "# MAKE SURE YOU RUN THIS 100 TIMES, NOT JUST ONE!!!\n",
    "for robot_index in tqdm(range(100)):\n",
    "    \n",
    "    for error_probability in range(0, 11):\n",
    "        \n",
    "        # generate error\n",
    "        error_test = correction.between(error_probability/100, data['robot' + str(robot_index)].copy())\n",
    "        \n",
    "        # cluster correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        cluster_correction = algo.cluster(np_array, line_ys)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), cluster_correction)\n",
    "        cluster_results[error_probability].append(percentage)\n",
    "        \n",
    "        # compare correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        compare_correction = algo.compare(np_array, word_centers)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), compare_correction)\n",
    "        compare_results[error_probability].append(percentage)\n",
    "        \n",
    "        # merge correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        merge_correction = algo.merge(np_array, line_ys)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), merge_correction)\n",
    "        merge_results[error_probability].append(percentage)\n",
    "        \n",
    "        # segment correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        segment_correction = algo.segment(np_array, line_ys)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), segment_correction)\n",
    "        segment_results[error_probability].append(percentage)\n",
    "        \n",
    "        # split correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        split_correction = algo.split(np_array, line_ys)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), split_correction)\n",
    "        split_results[error_probability].append(percentage)\n",
    "        \n",
    "        # stretch correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        stretch_correction = algo.stretch(np_array, line_ys)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), stretch_correction)\n",
    "        stretch_results[error_probability].append(percentage)\n",
    "        \n",
    "        # attach correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        attach_correction = algo.attach(np_array, line_ys)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), attach_correction)\n",
    "        attach_results[error_probability].append(percentage)\n",
    "\n",
    "        # chain correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        chain_correction = algo.chain(np_array, line_ys)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), chain_correction)\n",
    "        chain_results[error_probability].append(percentage)\n",
    "        \n",
    "        # regress correction\n",
    "        np_array = np.array(error_test.copy())\n",
    "        regress_correction = algo.regress(np_array, line_ys)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), regress_correction)\n",
    "        regress_results[error_probability].append(percentage)\n",
    "        \n",
    "        # warp correction\n",
    "        np_array = np.array(error_test.copy(), dtype=int)\n",
    "        durations = np.delete(np_array, 0, 1)\n",
    "        durations = np.delete(durations, 0, 1)\n",
    "        np_array = np.delete(np_array, 2, 1)\n",
    "        \n",
    "        warp_correction = algo.warp(np_array, word_centers)\n",
    "        percentage, match_list = correction.correction_quality(aoi, data['robot' + str(robot_index)].copy(), warp_correction)\n",
    "        warp_results[error_probability].append(percentage)\n",
    "\n",
    "        if percentage < 1:\n",
    "            correction.draw_correction('synthetic_text.png', warp_correction, match_list)\n",
    "            print(\"robot:\", robot_index, \"    error probability:\", error_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def mean_error_group(results):\n",
    "    new_list = []\n",
    "    \n",
    "    for result in results:\n",
    "        new_list.append(statistics.mean(result))\n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_attach_results = mean_error_group(attach_results)\n",
    "mean_chain_results = mean_error_group(chain_results)\n",
    "mean_regress_results = mean_error_group(regress_results)\n",
    "mean_warp_results = mean_error_group(warp_results)\n",
    "\n",
    "mean_cluster_results = mean_error_group(cluster_results)\n",
    "mean_compare_results = mean_error_group(compare_results)\n",
    "mean_segment_results = mean_error_group(segment_results)\n",
    "mean_split_results = mean_error_group(split_results)\n",
    "mean_stretch_results = mean_error_group(stretch_results)\n",
    "mean_merge_results = mean_error_group(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(mean_attach_results, color='Grey', linewidth=1, marker='^')\n",
    "plt.plot(mean_chain_results, color='Pink', linewidth=1, marker='o')\n",
    "plt.plot(mean_regress_results, color='Green', linewidth=1, marker='s')\n",
    "plt.plot(mean_warp_results, color='DarkViolet', linewidth=1, marker='d')\n",
    "\n",
    "plt.plot(mean_cluster_results, color='LightBlue', linewidth=1, marker='8')\n",
    "plt.plot(mean_compare_results, color='Orange', linewidth=1, marker='p')\n",
    "plt.plot(mean_segment_results, color='Yellow', linewidth=1, marker='P')\n",
    "plt.plot(mean_split_results, color='Red', linewidth=1, marker='*')\n",
    "plt.plot(mean_stretch_results, color='Black', linewidth=1, marker='h')\n",
    "plt.plot(mean_merge_results, color='DarkBlue', linewidth=1, marker='X')\n",
    "\n",
    "plt.legend(['attach', 'chain', 'regress', 'warp', 'cluster', 'segment', 'split', 'stretch', 'merge'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('between line regression')\n",
    "\n",
    "x_ticks_labels = ['0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0']\n",
    "plt.xticks(range(0, 11), x_ticks_labels)\n",
    "plt.savefig(\"between_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean attach:\", statistics.mean(mean_attach_results))\n",
    "print(\"mean chain:\", statistics.mean(mean_chain_results))\n",
    "print(\"mean regress:\", statistics.mean(mean_regress_results))\n",
    "print(\"mean warp:\", statistics.mean(mean_warp_results))\n",
    "\n",
    "print(\"mean cluster:\", statistics.mean(mean_warp_results))\n",
    "print(\"mean compare:\", statistics.mean(mean_compare_results))\n",
    "print(\"mean segment:\", statistics.mean(mean_warp_results))\n",
    "print(\"mean split:\", statistics.mean(mean_warp_results))\n",
    "print(\"mean stretch:\", statistics.mean(mean_warp_results))\n",
    "print(\"mean merge:\", statistics.mean(mean_warp_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
